import sys
import streamlit as st
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableMap
# from langchain_community.llms import ChatOllama
from langchain_community.chat_models import ChatOllama
from langchain_core.documents import Document

st.write(f"Python version: {sys.version}")

# ë¬¸ì„œ ì˜ˆì‹œ - ìŠ¤í¬ì¸  ê²½ë§¤ ë°ì´í„° ì˜ˆì‹œ (ì—¬ê¸°ì„  ì„ì‹œë¡œ ì„¤ì •)
sports_auction_info = [
    {
        "ê²½ë§¤ë²ˆí˜¸": "2025-07-16-001",
        "ì¢…ëª©": "ì¶•êµ¬",
        "ì„ ìˆ˜ëª…": "í™ê¸¸ë™",
        "ì†Œì†íŒ€": "ì„œìš¸FC",
        "ê²½ë§¤ì‹œì‘ê°€": "1ì–µì›",
        "ë§ˆê°ì¼": "2025-07-20",
        "íŠ¹ì´ì‚¬í•­": "Kë¦¬ê·¸ ë“ì ì™• ìˆ˜ìƒ ê²½ë ¥ ìˆìŒ"
    },
    {
        "ê²½ë§¤ë²ˆí˜¸": "2025-07-16-002",
        "ì¢…ëª©": "ì•¼êµ¬",
        "ì„ ìˆ˜ëª…": "ê¹€ì² ìˆ˜",
        "ì†Œì†íŒ€": "ë¶€ì‚°ë² ì–´ìŠ¤",
        "ê²½ë§¤ì‹œì‘ê°€": "1ì–µ 5ì²œë§Œì›",
        "ë§ˆê°ì¼": "2025-07-22",
        "íŠ¹ì´ì‚¬í•­": "ê³¨ë“ ê¸€ëŸ¬ë¸Œ 3íšŒ ìˆ˜ìƒì"
    }
]

# ë¬¸ì„œí™”
documents = [
    Document(
        page_content=", ".join([
            f"{key}: {value}" for key, value in item.items()
        ])
    )
    for item in sports_auction_info
]

# ì„ë² ë”© & ë²¡í„°DB ìƒì„±
# ì„ë² ë”© & ë²¡í„°DB ìƒì„±
embedding_function = HuggingFaceEmbeddings(
    model_name="jhgan/ko-sroberta-multitask"
)
db = FAISS.from_documents(documents, embedding_function)

retriever = db.as_retriever(search_type="similarity", search_kwargs={'k': 5})

# Ollama ëª¨ë¸ ì„¤ì •
llm = ChatOllama(model="gemma3", temperature=0.5, base_url="http://127.0.0.1:11434/")

template = """
ë„ˆëŠ” DropWin ì›¹ ê²½ë§¤ ì‚¬ì´íŠ¸ì˜ ìŠ¤í¬ì¸  ê²½ë§¤ ì „ë¬¸ ìƒë‹´ ì§ì›ì´ì•¼.
ì‚¬ìš©ìì™€ ëŒ€í™”í•  ë•ŒëŠ” ë°˜ë“œì‹œ ì •ì¤‘í•˜ê³  ì¹œì ˆí•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•´.
ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œ í•´ì¤˜.

ğŸ“Œ ë‹¤ìŒê³¼ ê°™ì€ ê¸°ì¤€ì„ ì§€ì¼œì„œ ë‹µë³€í•´ ì¤˜:
1. ì§ˆë¬¸ì— ìµœëŒ€í•œ ì •í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•´.
2. ìˆ«ì, ë‚ ì§œ, ê²½ë§¤ ì •ë³´ê°€ ìˆìœ¼ë©´ ì •í™•íˆ ì „ë‹¬í•´.
3. ë§Œì•½ ë°ì´í„°ì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìœ¼ë©´ "í•´ë‹¹ ì •ë³´ëŠ” í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤"ë¼ê³  ì •ì¤‘í•˜ê²Œ ì•ˆë‚´í•´.
4. ë‹µë³€ ë§ˆì§€ë§‰ì—ëŠ” ê°„ë‹¨í•œ ì•ˆë‚´ ë¬¸êµ¬(ì˜ˆ: "ë” ê¶ê¸ˆí•œ ê²Œ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”!")ë¥¼ ì¶”ê°€í•´.

ğŸ“ ì•„ë˜ëŠ” ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì°¸ê³  ì •ë³´ì•¼. ì´ ì •ë³´ë§Œ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´.
ë§Œì•½ ì°¸ê³  ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì´ë”ë¼ë„ ê±°ì§“ìœ¼ë¡œ ëŒ€ë‹µí•˜ì§€ ë§ê³ , ì†”ì§í•˜ê²Œ ì—†ë‹¤ê³  ë§í•´ì¤˜.

<ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì°¸ê³  ì •ë³´>
{context}

ì§ˆë¬¸: {question}
"""

prompt = ChatPromptTemplate.from_template(template)

# ì²´ì¸ êµ¬ì„±
chain = RunnableMap({
    "context": lambda x: retriever.get_relevant_documents(x["question"]),
    "question": lambda x: x["question"]
}) | prompt | llm

# --- Streamlit UI ì‹œì‘ ---
st.title("ğŸ… ìŠ¤í¬ì¸  ê²½ë§¤ ì±—ë´‡")
st.write("ìŠ¤í¬ì¸  ìŠ¤íƒ€ ê²½ë§¤ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

# ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# ìƒˆ ì±„íŒ… ë²„íŠ¼
if st.button("ğŸ”„ ìƒˆ ì±„íŒ… ì‹œì‘"):
    st.session_state.chat_history = []
    st.rerun()

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

# ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬
if user_input:
    response = chain.invoke({'question': user_input}).content
    st.session_state.chat_history.append(("ğŸ™‚ ì‚¬ìš©ì", user_input))
    st.session_state.chat_history.append(("ğŸ¤– ì±—ë´‡", response))

# ì´ì „ ì±„íŒ… ë‚´ì—­ ì¶œë ¥
if st.session_state.chat_history:
    st.markdown("---")
    st.subheader("ğŸ’¬ ì´ì „ ëŒ€í™”")
    for speaker, message in st.session_state.chat_history:
        st.markdown(f"**{speaker}:** {message}")